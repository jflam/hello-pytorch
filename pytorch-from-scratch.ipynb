{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch from scratch\n",
    "\n",
    "This uses a super-simple __linear regression__ model as the frame for walking from manually doing everything from good-old Python to using `numpy` to using `pytorch` on a GPU. It's based on this [excellent tutorial on pytorch](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this interesting equation we will train a linear regression model against some synthetic data. \n",
    "\n",
    "$$\n",
    "y = a + b x + \\epsilon\n",
    "$$\n",
    "\n",
    "We will start from a plain Python program (using only the `math` library), progress to a `numpy` implementation before showing how to do the same thing using `pytorch`. The goal is to give you visibility into some of the things that different libraries abstract away for you while simultaneously ensuring that you understand what those abstractions are. We will also have some fun looking at how libraries like [sympy](https://www.sympy.org/en/index.html) can be used to help with some of the more tedious aspects of the math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating synthetic data to train against\n",
    "\n",
    "Next, let's generate some synthetic data for an equation where `a = 1` and `b = 2`. The synthetic data will sprinkle in some Gaussian noise into the synthetic values. Let's begin by importing `numpy` and initializing the random number generator with a fixed seed value so that our results are reproducible in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a matrix of size [100, 1] of random numbers between 0 and 1 using the [random.rand() function](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.rand.html). Note that I'm using a naming convention in this notebook where variables prefixed with `d` represent __data__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012],\n",
       "       [0.95071431],\n",
       "       [0.73199394],\n",
       "       [0.59865848],\n",
       "       [0.15601864]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = np.random.rand(100, 1)\n",
    "dx[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute y values and sprinkle in the random noise. Marvel at the beautiful vector math and the compact notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.75778494],\n",
       "       [2.87152788],\n",
       "       [2.47316396],\n",
       "       [1.99856008],\n",
       "       [1.29007009]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy = 1 + 2 * dx + .1 * np.random.randn(100, 1)\n",
    "dy[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a sequence from 0 to 99 that will represent indices into our matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.arange(100)\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now shuffle so that the indices are in a random order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([76, 83, 80, 98,  2, 77, 71, 84, 89, 50, 40, 51, 67, 86, 37, 49,  4,\n",
       "       10, 69, 81,  9, 54, 55, 87, 64, 44, 90, 75, 33, 30, 93, 95, 14, 61,\n",
       "       11, 13, 15,  7,  0, 19, 35,  6, 12, 65, 70, 88, 56, 58, 28, 38, 91,\n",
       "       42,  8, 73, 39, 85, 25, 92, 41, 26,  1, 22, 21, 46, 74, 79, 78, 72,\n",
       "       57, 53, 24, 17, 66, 32, 31, 62, 59, 52, 82, 23, 36,  5, 45, 99, 43,\n",
       "       16, 48, 94, 34,  3, 18, 47, 60, 68, 63, 27, 96, 29, 20, 97])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the indices for the training and validation dataset such that first 80 random indices are used for training and the last 20 random indices are used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([76, 83, 80, 98,  2, 77, 71, 84, 89, 50, 40, 51, 67, 86, 37, 49,  4,\n",
       "       10, 69, 81,  9, 54, 55, 87, 64, 44, 90, 75, 33, 30, 93, 95, 14, 61,\n",
       "       11, 13, 15,  7,  0, 19, 35,  6, 12, 65, 70, 88, 56, 58, 28, 38, 91,\n",
       "       42,  8, 73, 39, 85, 25, 92, 41, 26,  1, 22, 21, 46, 74, 79, 78, 72,\n",
       "       57, 53, 24, 17, 66, 32, 31, 62, 59, 52, 82, 23])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = idx[:80]\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36,  5, 45, 99, 43, 16, 48, 94, 34,  3, 18, 47, 60, 68, 63, 27, 96,\n",
       "       29, 20, 97])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx = idx[80:]\n",
    "val_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the training and validation datasets. Notice how compact this is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_train, dy_train = dx[train_idx], dy[train_idx]\n",
    "dx_val, dy_val = dx[val_idx], dy[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot these two datasets side-by-side so we can see the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"66ae74b7-6fab-445e-97ea-5e3cb83d180b\" data-root-id=\"1080\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"40b33c71-aa76-4c5a-8ee4-5bac7fde4c64\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"id\":\"1041\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"1080\",\"type\":\"Row\"},{\"attributes\":{\"ticker\":{\"id\":\"1014\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"Grid\"},{\"attributes\":{\"ticker\":{\"id\":\"1053\",\"type\":\"BasicTicker\"}},\"id\":\"1056\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1058\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis_label\":\"y\",\"formatter\":{\"id\":\"1090\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1058\",\"type\":\"BasicTicker\"}},\"id\":\"1057\",\"type\":\"LinearAxis\"},{\"attributes\":{\"overlay\":{\"id\":\"1091\",\"type\":\"BoxAnnotation\"}},\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1084\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1086\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"ResetTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"marker\":{\"value\":\"square\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1077\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1088\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"text\":\"Training dataset\"},\"id\":\"1042\",\"type\":\"Title\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1062\",\"type\":\"PanTool\"},{\"id\":\"1063\",\"type\":\"WheelZoomTool\"},{\"id\":\"1064\",\"type\":\"BoxZoomTool\"},{\"id\":\"1065\",\"type\":\"SaveTool\"},{\"id\":\"1066\",\"type\":\"ResetTool\"},{\"id\":\"1067\",\"type\":\"HelpTool\"}]},\"id\":\"1068\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1028\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1090\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1023\",\"type\":\"PanTool\"},{\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"id\":\"1026\",\"type\":\"SaveTool\"},{\"id\":\"1027\",\"type\":\"ResetTool\"},{\"id\":\"1028\",\"type\":\"HelpTool\"}]},\"id\":\"1029\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"1075\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1076\",\"type\":\"Scatter\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1077\",\"type\":\"Scatter\"},\"selection_glyph\":null,\"view\":{\"id\":\"1079\",\"type\":\"CDSView\"}},\"id\":\"1078\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1091\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1062\",\"type\":\"PanTool\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":{\"__ndarray__\":\"6rIgwMp+0z9Mf5nhoPfDP3Z9Be9hM+U/0Ht0xsWeuz8xcpAZJxntP2iq5HS0eNM/PFNFjqZ+4T/GNgZdw6voP6DOQCZ15u4/sCxj1jUo4z9aOy+4/KTbP6ju1LBlpOA/9oL6tRbg2D/oF4BAwBWzP8bXuOcL1dY/E9Tux5t04D8DuNcyOrrgP3BDCTdZyKc/J3WZhUyU4z+6GuMA1VzbPw==\",\"dtype\":\"float64\",\"shape\":[20]},\"y\":{\"__ndarray__\":\"Im9z+BDY+T9FY/8vLpD1P2KOtN9y3wFAGWoSLKnG9D96jxt/kiYHQGEeNaDKnPg/mea2fYM3AUAmde+BtZ8DQMTU0bND6gVAAF2YIRr6/z9Z5gDX4jH9Pz8+5/MyZv4/cCWFSzkv+z8VW7O3Ks3yP/uVdHlccvk/zgR56f4r/z89isu/Uk0AQOwUCmEFIvI/yEThfcsGAkDMPAnhJjr+Pw==\",\"dtype\":\"float64\",\"shape\":[20]}},\"selected\":{\"id\":\"1096\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1095\",\"type\":\"UnionRenderers\"}},\"id\":\"1075\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1053\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1063\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"source\":{\"id\":\"1075\",\"type\":\"ColumnDataSource\"}},\"id\":\"1079\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1092\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis_label\":\"x\",\"formatter\":{\"id\":\"1084\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1014\",\"type\":\"BasicTicker\"}},\"id\":\"1013\",\"type\":\"LinearAxis\"},{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1022\",\"type\":\"Grid\"},{\"attributes\":{\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"#1f77b4\"},\"marker\":{\"value\":\"square\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1037\",\"type\":\"Scatter\"},{\"attributes\":{\"overlay\":{\"id\":\"1094\",\"type\":\"BoxAnnotation\"}},\"id\":\"1064\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1093\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null},\"id\":\"1044\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":{\"__ndarray__\":\"xYBsJj+u6D84vt0rXEWwP4NwZhOLnus/oC08uHgHmj9RZx6PfmznP7hBpB2X9LI/qCCy9INvyT9wyVllIufTP/Y7EPPEON4/5n9eV9YG7z+wF4XT5T2/P1iMzFnjzeg/vDupAJmr6T8qfV0O71jnP3B7YycKAbk/XEKAkE+pxz+EISI3a/jDP4B+OAcaFJU/KJCx6ZOU7z/MPMzpDvLjP4KCmtKHqOY//bnoIv8h4z+0YE5l/n/tP6tQGe3eZuQ/xDTbvNT60T8wOHXp2Y/QP/BDGH+6nb4/xiMp1wZU5z+J0MQzRV3uP4qiuOYBceM/B6bxmPv14T/YzlLYWJrfPzgZ4F8KRsc/GkpRVMhd0T+lU+NigAnvP5gwA5DtLcs/SDThicx5xz+1t1UJt7frP+xRXx53+Nc/Dr0yjH+j0j+fqoodZN7pP3BVZJ0iva0/9hPBv16j6j+HnxotxF3hP9JblKk6tug/mLY++gtk7D+4E0HScae2P6DXg/0HKKc/F/fdZQ/14j9hz/aoPOXlP6pJZbvm0uY/AGgYS1+boT+tS5KMVTzjP1edy5BCGOo/3FJ3XXUr3D8Yz1O1zc/UPy46eVYpIOk/EA3j4llY6D8YYqp++rDfP3SQHhbpjsk/VNa7aEBs7j9UQMp3f7LSP3wkv1Dv2sE/HrhmAxPz0z8GathKk57mP4BZRz2Yqb0/3mc1PRrx1j8AeDUHXJ52P0xWVWj3Fck/D+a092yi7D/cj7koQDDdP2M8BwDOyuA/SIm9FM4Jwj+gqmygOKewP7Q+NQe808U/TNVYhgSF6j/2msFONtLUP0OilBVgEO4/YGbD6G4t1T84t9zxeHLXPw==\",\"dtype\":\"float64\",\"shape\":[80]},\"y\":{\"__ndarray__\":\"1YfmsdrLA0CuKTcomBXzP6fhSNthTQdAnaTY55BB8T9Kn9ovCskDQDNsanS0vfI/XSjeEwU/9D+arNBqVmT4P1GvKrF1xf8/ZLMtio64BkBokHfvhNnzPw6Fb1338gNAzesHopz7BEBjpc9Nv5kEQJPX5OwSEfc/RvmZz4lr+T/S83GJIKT0P67qtq6TH/I/YTjN/WdqCEDcYrXyEVoCQH0RQ2SB7QJAmokxJWxTAECqy1LfCc4GQAp67hJciwFACQuORZVa+T/i1d6q7ov5PwoRR5FuJPU/KzW8SPfwA0AAlLd9Yx8HQMY3/GnLOgNA8T3sUKBgAEBriZODBH/+P/psl+lF+fU/W5kpkfkq+D/kRSU4FMgHQCa3v2G3nfc/sIlx7DRr9z9Zeow7t3EFQKa7IRvjH/w/H0baG0v69j9C+o73w+kEQBDcBJYqOfQ/G57IrjDlBEAb/Z0DlroBQOswYn3JXQNAEnCmpFL3BkAZ8BmczyHxP8x9+cLP8+8/03LBrn9ZAUALMlETOcsCQGLeKPLp7QRAoxEHnc/t8j/6YtZslvgAQJb3NNXzSAVApEqRNj6R/j/QhB2oSqr7PwIFSb3uOQRA+BPniev5A0B3YiN5zPn9PwMWxwxa1/U//rSkm+P4BkCWmjPrV1v5P4wfSnWp4fQ/ru/AJh84/D8MTYyvloIDQPaty9qZkPI/sMjn/pfw+z+PA0ZCBgPxP7RFF555B/c/uHAD3BnqBUCHqk6zY1T8PyqZIBFMIgBAQEqtrwbw8T8F8rUxZX7yPywQSNFwvPU/yhVv/h3pBUB1mEGO9eP8PzmPxaSYHAdA8ouhOsKu+D9u54Q6Jln7Pw==\",\"dtype\":\"float64\",\"shape\":[80]}},\"selected\":{\"id\":\"1093\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1092\",\"type\":\"UnionRenderers\"}},\"id\":\"1036\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1065\",\"type\":\"SaveTool\"},{\"attributes\":{\"axis_label\":\"y\",\"formatter\":{\"id\":\"1086\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1094\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1095\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null},\"id\":\"1046\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"marker\":{\"value\":\"square\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1038\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1096\",\"type\":\"Selection\"},{\"attributes\":{\"below\":[{\"id\":\"1052\",\"type\":\"LinearAxis\"}],\"center\":[{\"id\":\"1056\",\"type\":\"Grid\"},{\"id\":\"1061\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1057\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":400,\"renderers\":[{\"id\":\"1078\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1042\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1068\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1044\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1048\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1046\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1050\",\"type\":\"LinearScale\"}},\"id\":\"1041\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"}},\"id\":\"1040\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"HelpTool\"},{\"attributes\":{\"text\":\"Training dataset\"},\"id\":\"1003\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"LinearScale\"},{\"attributes\":{\"data_source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1037\",\"type\":\"Scatter\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1038\",\"type\":\"Scatter\"},\"selection_glyph\":null,\"view\":{\"id\":\"1040\",\"type\":\"CDSView\"}},\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_color\":{\"value\":\"red\"},\"line_color\":{\"value\":\"#1f77b4\"},\"marker\":{\"value\":\"square\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1076\",\"type\":\"Scatter\"},{\"attributes\":{\"below\":[{\"id\":\"1013\",\"type\":\"LinearAxis\"}],\"center\":[{\"id\":\"1017\",\"type\":\"Grid\"},{\"id\":\"1022\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1018\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":400,\"renderers\":[{\"id\":\"1039\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1003\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1029\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1005\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1009\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1007\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1011\",\"type\":\"LinearScale\"}},\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis_label\":\"x\",\"formatter\":{\"id\":\"1088\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1053\",\"type\":\"BasicTicker\"}},\"id\":\"1052\",\"type\":\"LinearAxis\"},{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1058\",\"type\":\"BasicTicker\"}},\"id\":\"1061\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"PanTool\"}],\"root_ids\":[\"1080\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n",
       "  var render_items = [{\"docid\":\"40b33c71-aa76-4c5a-8ee4-5bac7fde4c64\",\"roots\":{\"1080\":\"66ae74b7-6fab-445e-97ea-5e3cb83d180b\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1080"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import row\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "plot_train = figure(title=\"Training dataset\", x_axis_label='x', y_axis_label='y', plot_height=400, plot_width=400)\n",
    "plot_train.scatter(dx_train.flatten(), dy_train.flatten(), marker=\"square\", fill_color=\"blue\")\n",
    "\n",
    "plot_validate = figure(title=\"Training dataset\", x_axis_label='x', y_axis_label='y', plot_height=400, plot_width=400)\n",
    "plot_validate.scatter(dx_val.flatten(), dy_val.flatten(), marker=\"square\", fill_color=\"red\")\n",
    "\n",
    "show(row(plot_train, plot_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "The loss function that we will use is the [mean square error]() loss function represented by the following equation:\n",
    "\n",
    "$$\n",
    "MSE=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat y_i)^2\n",
    "$$\n",
    "\n",
    "Recall that $\\hat y_i$ is computed by the general linear regression equation:\n",
    "\n",
    "$$\n",
    "\\hat y_i = a - b x_i\n",
    "$$\n",
    "\n",
    "which yields:\n",
    "\n",
    "$$\n",
    "MSE=\\frac{1}{N}\\sum_{i=1}^N (y_i - a - b x_i)^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to compute the gradients, which are a partial derivative of each of the two variables that we are trying to learn: `a` and `b`. The following equations are computed through the application of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "\n",
    "<div id=\"partialderivatives\">\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial MSE}{\\partial a} & = \n",
    "\\frac {\\partial MSE}{\\partial \\hat y_i} \\cdot\n",
    "\\frac {\\partial \\hat y_i}{\\partial a} \\\\\\\\ & =\n",
    "\\frac {1}{N} \n",
    "\\sum_ {i=1}^N 2(y_i - a - b x_i) \\cdot (-1) \\\\\\\\ & =\n",
    "-2 \\frac {1}{N} \\sum_ {i=1}^N (y_i - \\hat y_i) \\\\\\\\\n",
    "\\frac {\\partial MSE}{\\partial b} & = \n",
    "\\frac {\\partial MSE}{\\partial \\hat y_i} \\cdot\n",
    "\\frac {\\partial \\hat y_i}{\\partial b} \\\\\\\\ & =\n",
    "\\frac {1}{N} \n",
    "\\sum_ {i=1}^N 2(y_i - a - b x_i) \\cdot (-x_i) \\\\\\\\ & =\n",
    "-2 \\frac {1}{N} \\sum_ {i=1}^N x_i (y_i - \\hat y_i)\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sympy to perform the same computation symbolically\n",
    "\n",
    "We can also use `sympy` to compute the equations that we will be using for our gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing the `sympy` library and declaring that `a`, `b` and `N` will be scalar symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Sum, IndexedBase, diff, Symbol\n",
    "from sympy.abc import x, y, i, a, b\n",
    "N = Symbol('N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will declare $x_i$ and $y_i$ as __indexed__ symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = IndexedBase('y')\n",
    "x = IndexedBase('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will define the __mean square error__ equation using a combination of Python arithmetic expressions and the sympy `Sum` method to define the summation expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\sum_{i=1}^{N} \\left(- a - b {x}_{i} + {y}_{i}\\right)^{2}}{N}$"
      ],
      "text/plain": [
       "Sum((-a - b*x[i] + y[i])**2, (i, 1, N))/N"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = (1/N) * Sum((y[i] - a - b * x[i]) ** 2, [i, 1, N])\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can compute the partial derivative of `mse` with respect to `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\sum_{i=1}^{N} \\left(2 a + 2 b {x}_{i} - 2 {y}_{i}\\right)}{N}$"
      ],
      "text/plain": [
       "Sum(2*a + 2*b*x[i] - 2*y[i], (i, 1, N))/N"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(mse, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the partial derivative of `mse` with respect to `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\sum_{i=1}^{N} - 2 \\left(- a - b {x}_{i} + {y}_{i}\\right) {x}_{i}}{N}$"
      ],
      "text/plain": [
       "Sum(-2*(-a - b*x[i] + y[i])*x[i], (i, 1, N))/N"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(mse, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same result as we got by manually applying the chain rule in the previous section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using learning rate to update parameters\n",
    "\n",
    "The $a$ and $b$ parameters need to be updated by $\\eta$ which is the __learning rate__. It is a multiplicative factor that we need to apply to the gradient to update the values of the parameters.\n",
    "\n",
    "Here are the equations that we will need to use to update the parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a = a - \\eta \\frac{\\partial MSE}{\\partial a} \\\\\\\\\n",
    "b = b - \\eta \\frac{\\partial MSE}{\\partial b}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "While training the model, we complete __epochs__. An [epoch](https://stackoverflow.com/questions/31155388/meaning-of-an-epoch-in-neural-networks-training) is complete whenever every point has already been used for computing the loss. \n",
    "\n",
    "__Batch__ gradient descent uses all points for computing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model using plain old Python\n",
    "\n",
    "TODO write a section that will train the model using nothing more than plain vanilla Python, i.e., no `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa = 0.6394267984578837, pb = 0.025010755222666936\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "pa = random.random()\n",
    "pb = random.random()\n",
    "print(f\"pa = {pa}, pb = {pb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1927720614354302"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_train[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6587168523093474,\n",
       " 0.641016440799244,\n",
       " 0.6610136669743035,\n",
       " 0.6400625500148542,\n",
       " 0.6577345197610039]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = [pa + pb * x_i[0] for x_i in dx_train]\n",
    "yhat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8158213689046947,\n",
       " 0.5517556206361862,\n",
       " 2.2517706358913596,\n",
       " 0.4384447834063381,\n",
       " 1.8154294415153562]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = [y_i[0] - yhat_i for y_i, yhat_i in zip(dy_train, yhat)]\n",
    "error[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.6051045660742225"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_grad = -2 * (sum(error) / len(error))\n",
    "a_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5953884817796105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter = [-2 * x_i[0] * error_i for x_i, error_i in zip(dx_train, error)]\n",
    "b_grad = sum(inter) / len(inter)\n",
    "b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899937255065306"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa = pa - lr * a_grad\n",
    "pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18454960340062798"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb = pb - lr * b_grad\n",
    "pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0235408899600786 1.9689642057454355\n"
     ]
    }
   ],
   "source": [
    "pa = random.random()\n",
    "pb = random.random()\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = [pa + pb * x_i[0] for x_i in dx_train]\n",
    "    error = [y_i[0] - yhat_i for y_i, yhat_i in zip(dy_train, yhat)]\n",
    "    a_grad = -2 * (sum(error) / len(error))\n",
    "    inter = [-2 * x_i[0] * error_i for x_i, error_i in zip(dx_train, error)]\n",
    "    b_grad = sum(inter) / len(inter)\n",
    "    pa = pa - lr * a_grad\n",
    "    pb = pb - lr * b_grad\n",
    "    \n",
    "print(pa, pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model using numpy\n",
    "\n",
    "Next, let's train the model using the `numpy` library which makes your code much more succinct as it lets you define __vectorized__ operations easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by randomly initializing our $a$ and $b$ parameters. Note that I'm using a naming convention in this notebook where I prefix __parameters__ with `p`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa = [0.49671415], pb = [-0.1382643]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "pa = np.random.randn(1)\n",
    "pb = np.random.randn(1)\n",
    "print(f\"pa = {pa}, pb = {pb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that type types of $a$ and $b$ are `np.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the learning rate. We won't say why we chose this value - choosing a learning rate turns out to be a very important training parameter to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that type type of `x_train` is `numpy.ndarray`. This is important as this will force all of the expressions below to be __vectorized__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute `yhat` using the simple vectorized expression below. Note that `pb` is a scalar and `dx_train` is a vector, which results in `yhat` becoming a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.390075  ],\n",
       "       [0.4879263 ],\n",
       "       [0.37737776],\n",
       "       [0.4931996 ],\n",
       "       [0.39550552]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = pa + pb * dx_train\n",
    "yhat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute `error` by computing the vector difference between `dy_train` and `yhat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.08446322],\n",
       "       [0.70484576],\n",
       "       [2.53540654],\n",
       "       [0.58530774],\n",
       "       [2.07765844]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = dy_train - yhat\n",
    "error[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute the __gradients__ for a and b, based on the [partial derivatives](#partialderivatives) that we computed earlier. First we have $\\frac {\\partial MSE}{\\partial a}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.044811379650508"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_grad = -2 * error.mean()\n",
    "a_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can do the same thing for $\\frac {\\partial MSE}{\\partial b}$. But here, note that the \"inner loop\" in this calculation is succinctly represented by the vectorized operation `dx_train * error`. The result, of course, is a scalar because we are computing the `mean` of the vector generated previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8337537171510832"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_grad = -2 * (dx_train * error).mean()\n",
    "b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the new values of the $a$ and $b$ __parameters__ by multiplying the gradients by the __learning rate__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80119529])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa = pa - lr * a_grad\n",
    "pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04511107])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb = pb - lr * b_grad\n",
    "pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of this together, we can write the loop that iterates over __1000 epochs__, taking care to initialize all of our parameters at the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354078] [1.96896442]\n"
     ]
    }
   ],
   "source": [
    "pa = np.random.randn(1)\n",
    "pb = np.random.randn(1)\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = pa + pb * dx_train\n",
    "    error = (dy_train - yhat)\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (dx_train * error).mean()\n",
    "    pa = pa - lr * a_grad\n",
    "    pb = pb - lr * b_grad\n",
    "    \n",
    "print(pa, pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we didn't make a mistake, let's check our result against what scikit-learn's [linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) computes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [[1.96896447]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(dx_train, dy_train)\n",
    "print(linr.intercept_, linr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's determine whether a GPU, i.e., [CUDA](https://en.wikipedia.org/wiki/CUDA) is available. If not, perform our computations on the CPU instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to import our `numpy` arrays into `pytorch` tensors. It's worth noting that pytorch refers to all non-scalar data as `tensors`, so a vector is a [tensor](https://en.wikipedia.org/wiki/Tensor), as is a matrix. Let's use the [torch.fromnumpy()](https://pytorch.org/docs/stable/torch.html) method to import our data from the numpy `dx_train`, `dy_train` vectors (really matrices). \n",
    "\n",
    "It's worth noting that this method appears to round to 5 significant digits for some reason as well, regardless of whether you explicitly convert via `float()` or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7713])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx_train_tensor = torch.from_numpy(dx_train).float().to(device)\n",
    "dy_train_tensor = torch.from_numpy(dy_train).float().to(device)\n",
    "dx_train_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `type()` API to get additional information about where the tensor is located (i.e., what device it resides on). I believe that the absence of a device in the output implies that it is located on the CPU, e.g., a tensor that resides on your GPU will return its type as `torch.cuda.FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx_train_tensor.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's initialize our $a$ and $b$ parameters. As before, we need to initialize a seed for torch's random number generator so we our results are reproducible. Then we need to create our parameters and assign them to the device that we picked earlier. A key thing that we're doing is setting a flag, `requires_grad=True` which tells pytorch to automatically compute gradients for us vs. the manual computation that we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "pa = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "pb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(pa, pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these parameters are also tensors, much like in the `numpy` example earlier where `pa` and `pb` were of type `numpy.ndarray`. This is required to take advantage of the autograd functionality in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the gradients computed? Pytorch contains an [autograd](https://pytorch.org/docs/stable/autograd.html) package that is automatically invoked when you enable `requires_grad=True` on your tensors. So rather than have to define (and compute) the partial derivatives $\\frac {\\partial MSE}{\\partial a}$ and $\\frac {\\partial MSE}{\\partial b}$, you just have to define the __loss function, MSE__ instead.\n",
    "\n",
    "Let's walk through the code that we wrote earlier for `numpy` line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4360],\n",
       "        [0.3449],\n",
       "        [0.4479],\n",
       "        [0.3400],\n",
       "        [0.4310]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = pa + pb * dx_train_tensor\n",
    "yhat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0385],\n",
       "        [0.8479],\n",
       "        [2.4649],\n",
       "        [0.7385],\n",
       "        [2.0422]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = dy_train_tensor - yhat\n",
    "error[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the loss function, MSE, as opposed to manually computing the gradient functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7475, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (error ** 2).mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we need to explicitly tell pytorch to compute the gradient by calling `backward` on the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the computed gradients? The autograd function knows how to extract that presumably from the computational graph that was generated in the equations that we defined earlier. This is cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.1125]) tensor([-1.8156])\n"
     ]
    }
   ],
   "source": [
    "print(pa.grad, pb.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's update our parameters $a$ and $b$ by multiplying our automatically computed gradients by our fixed learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6479], requires_grad=True) tensor([0.3104], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pa -= lr * pa.grad\n",
    "    pb -= lr * pb.grad\n",
    "    \n",
    "print(pa, pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, gradients are __accumulated__, so we need to explicitly zero out the gradients on each iteration.\n",
    "\n",
    "Note that any pytorch methods that end in `_` are __in-place__ operations. Make sure you use this variant over the ones that do not end in `_`, as those methods will return a __new object__ which is not what we want here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "pa.grad.zero_()\n",
    "pb.grad.zero_()\n",
    "\n",
    "print(pa.grad, pb.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take everything we've learned here and write a loop that will do the same training using pytorch over 1000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "pa = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "pb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = pa + pb * dx_train_tensor\n",
    "    error = dy_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        pa -= lr * pa.grad\n",
    "        pb -= lr * pb.grad\n",
    "    pa.grad.zero_()\n",
    "    pb.grad.zero_()\n",
    "    \n",
    "print(pa, pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the pytorch dyanmic computation graph using the [pytorchviz](https://github.com/szagoruyko/pytorchviz) package and its `make_dot()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invoke `make_dot` method across arbitrary expressions that define pytorch's dynamic compute graph. For the yhat expression, recall that the equation is:\n",
    "\n",
    "$$\n",
    "\\hat y_i = a + b x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 140668299536592 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140668299536592</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299536144 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140668299536144</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299536144&#45;&gt;140668299536592 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140668299536144&#45;&gt;140668299536592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 140668299537296 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140668299537296</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299537296&#45;&gt;140668299536592 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140668299537296&#45;&gt;140668299536592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 140668299535440 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140668299535440</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299535440&#45;&gt;140668299537296 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140668299535440&#45;&gt;140668299537296</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fefe405b490>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph depicts the flow of the gradient computations. It is intended to be read from the bottom up.\n",
    "\n",
    "The green box represents the starting point for the computation of gradients. The blue boxes are the parameters that we are training __and have autograd enabled__. We are missing the labels for the boxes (for now - we will see how to do this later when we define an actual model), but the top blue box represents $b$, and the blue box to the left represents $a$. \n",
    "\n",
    "The green box adds $a$ to the result of multiplying $b$ against the training tensor `dx_train_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"228pt\"\n",
       " viewBox=\"0.00 0.00 171.50 228.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 224)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-224 167.5,-224 167.5,4 -4,4\"/>\n",
       "<!-- 140668299513360 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140668299513360</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"117,-21 27,-21 27,0 117,0 117,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299536592 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140668299536592</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118,-78 26,-78 26,-57 118,-57 118,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299536592&#45;&gt;140668299513360 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140668299536592&#45;&gt;140668299513360</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72,-56.7787C72,-49.6134 72,-39.9517 72,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-31.1732 72,-21.1732 68.5001,-31.1732 75.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 140668299536144 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140668299536144</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-149 0,-149 0,-114 54,-114 54,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299536144&#45;&gt;140668299536592 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140668299536144&#45;&gt;140668299536592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-113.6724C45.4798,-105.2176 52.5878,-95.1085 58.6352,-86.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-88.4169 64.4601,-78.2234 55.8452,-84.3906 61.5714,-88.4169\"/>\n",
       "</g>\n",
       "<!-- 140668299537296 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140668299537296</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-142 72.5,-142 72.5,-121 163.5,-121 163.5,-142\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-128.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299537296&#45;&gt;140668299536592 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140668299537296&#45;&gt;140668299536592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-120.9317C103.7191,-111.6309 93.821,-97.8597 85.7479,-86.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-84.3753 79.761,-78.2979 82.7553,-88.4608 88.4395,-84.3753\"/>\n",
       "</g>\n",
       "<!-- 140668299535440 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140668299535440</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-220 91,-220 91,-185 145,-185 145,-220\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299535440&#45;&gt;140668299537296 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140668299535440&#45;&gt;140668299537296</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-184.9494C118,-175.058 118,-162.6435 118,-152.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-152.0288 118,-142.0288 114.5001,-152.0289 121.5001,-152.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fefe3ff92d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"342pt\"\n",
       " viewBox=\"0.00 0.00 171.50 342.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 338)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-338 167.5,-338 167.5,4 -4,4\"/>\n",
       "<!-- 140668299089808 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140668299089808</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"121,-21 23,-21 23,0 121,0 121,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299089360 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140668299089360</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118.5,-78 25.5,-78 25.5,-57 118.5,-57 118.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299089360&#45;&gt;140668299089808 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140668299089360&#45;&gt;140668299089808</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72,-56.7787C72,-49.6134 72,-39.9517 72,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-31.1732 72,-21.1732 68.5001,-31.1732 75.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 140668299513360 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140668299513360</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"117,-135 27,-135 27,-114 117,-114 117,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299513360&#45;&gt;140668299089360 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140668299513360&#45;&gt;140668299089360</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72,-113.7787C72,-106.6134 72,-96.9517 72,-88.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-88.1732 72,-78.1732 68.5001,-88.1732 75.5001,-88.1732\"/>\n",
       "</g>\n",
       "<!-- 140668299536592 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140668299536592</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"118,-192 26,-192 26,-171 118,-171 118,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299536592&#45;&gt;140668299513360 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140668299536592&#45;&gt;140668299513360</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72,-170.7787C72,-163.6134 72,-153.9517 72,-145.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.5001,-145.1732 72,-135.1732 68.5001,-145.1732 75.5001,-145.1732\"/>\n",
       "</g>\n",
       "<!-- 140668299536144 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140668299536144</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-263 0,-263 0,-228 54,-228 54,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299536144&#45;&gt;140668299536592 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140668299536144&#45;&gt;140668299536592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-227.6724C45.4798,-219.2176 52.5878,-209.1085 58.6352,-200.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-202.4169 64.4601,-192.2234 55.8452,-198.3906 61.5714,-202.4169\"/>\n",
       "</g>\n",
       "<!-- 140668299537296 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140668299537296</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-256 72.5,-256 72.5,-235 163.5,-235 163.5,-256\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299537296&#45;&gt;140668299536592 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140668299537296&#45;&gt;140668299536592</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-234.9317C103.7191,-225.6309 93.821,-211.8597 85.7479,-200.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-198.3753 79.761,-192.2979 82.7553,-202.4608 88.4395,-198.3753\"/>\n",
       "</g>\n",
       "<!-- 140668299535440 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140668299535440</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-334 91,-334 91,-299 145,-299 145,-334\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-306.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299535440&#45;&gt;140668299537296 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140668299535440&#45;&gt;140668299537296</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-298.9494C118,-289.058 118,-276.6435 118,-266.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-266.0288 118,-256.0288 114.5001,-266.0289 121.5001,-266.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fefe3ff9e10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently manually updating our training parameters $a$ and $b$, much like what we did for the `numpy` implementation. Instead of doing it by hand, we can use a pytorch __optimizer__. Since we are performing a batch gradient descent we can use pytorch's [Stochastic Gradient Descent optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) in place of our code.\n",
    "\n",
    "We need to initialize the optimizer with the names of our training parameters (which will be inserted into the pytorch dynamic computation graph) as well as our choice of the __learning rate__. Let's rewrite our previous program to explicitly use the SGD optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "torch.manual_seed(42)\n",
    "pa = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "pb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = SGD([pa, pb], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = pa + pb * dx_train_tensor\n",
    "    error = dy_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(pa, pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in place of the manual calculation of updated values for `pa` and `pb`, we call `optimizer.step()` instead, which does that operation for us. Zeroing of the gradients is still an explicit step, but we no longer need to specify `pa` and `pb`; the optimizer already knows that those are the parameters that it is optimizing for as they were initialized during construction of the `optimizer` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch models\n",
    "\n",
    "Pytorch lets you organize your code into a Python class vs. defining everything at global scope. In return for this ceremony, it provides these benefits:\n",
    "\n",
    "- a dictionary of all of the parameters used by the model (we'll see how this is useful in our graphs)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "class ManualLinearRegression(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pa = Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.pb = Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, dx):\n",
    "        return self.pa + self.pb * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `model` as well as a set of helper functions like the `MSELoss` function to avoid implementing your own loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235]) tensor([1.9690])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = MSELoss(reduction='mean')\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    yhat = model(dx_train_tensor)\n",
    "    loss = loss_fn(dy_train_tensor, yhat)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict()['pa'], model.state_dict()['pb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of the next few examples that follow, let's refactor all of the code that __uses the model__ into a function that we can reuse in our subsequent examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD\n",
    "\n",
    "def train_linear_model(model, dx, dy):\n",
    "    lr = 1e-1\n",
    "    EPOCHS = 1000\n",
    "    loss_fn = MSELoss(reduction='mean')\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        yhat = model(dx)\n",
    "        loss = loss_fn(dy, yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `train_model` with our `ManualLinearRegression` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('pa', tensor([1.0235])), ('pb', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "model = ManualLinearRegression().to(device)\n",
    "train_linear_model(model, dx_train_tensor, dy_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 140668299145936 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140668299145936</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299090832 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140668299090832</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299090832&#45;&gt;140668299145936 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140668299090832&#45;&gt;140668299145936</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 140668299089616 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140668299089616</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140668299089616&#45;&gt;140668299145936 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140668299089616&#45;&gt;140668299145936</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 140668299145552 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140668299145552</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140668299145552&#45;&gt;140668299089616 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140668299145552&#45;&gt;140668299089616</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fefe3ffcdd0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(model.forward(dx_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you don't even need to create a linear model, pytorch has a [Linear Model](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) already.\n",
    "\n",
    "Let's use it by constructing a new class `LayeredLinearRegression` that simply delegates to the underlying `Linear` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "class LayeredLinearRegression(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(1,1)\n",
    "    \n",
    "    def forward(self, dx):\n",
    "        return self.linear(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it automatically constructs parameters for the intercept and slope (weight and bias) with `requires_grad=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7388]], requires_grad=True), Parameter containing:\n",
       " tensor([0.1354], requires_grad=True)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LayeredLinearRegression()\n",
    "[*model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's replace our code from before that used `ManualLinearRegression` with our new `LayeredLinearRegression` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[1.9690]])), ('linear.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "model = LayeredLinearRegression().to(device)\n",
    "train_linear_model(model, dx_train_tensor, dy_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the weight ($b$) and bias ($a$) parameters are the same as we got from our `ManualLinearRegression` type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why define a separate `LayeredLinearRegression` class to simply forward computations to a `Linear` class? In neural networks, there are models that sequentially feed the output of one layer as the input to the next. pytorch provides a model type called [Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) model that does exactly this. \n",
    "\n",
    "Here is some code that constructs a new `Sequential` model and uses it to make the same set of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Sequential\n",
    "\n",
    "model = Sequential(Linear(1, 1)).to(device)\n",
    "train_linear_model(model, dx_train_tensor, dy_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the names of the parameters are a more generic `0.weight` and `0.bias` representing the underlying layer and the parameter name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a step back to generalize\n",
    "\n",
    "There is a set of operations that need to be completed during each training step. We can write a function `train_step` that is a further generalization of the `train_model` function that abstracts away the training loop in a way that lets us parameterize the __model__, the __loss function__, and the __optimizer__. Let's see what we can do here with the function `make_train_step_function` which accepts a model, a loss function and an optimizer as the parameters.\n",
    "\n",
    "What is unclear to me right now is whether the `loss` object returned by the `loss_fn` is something that is generalizable. But I get the impression that this is a key abstraction within the pytorch framework to represent this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step_function(model, loss_fn, optimizer):\n",
    "    def train_step(dx, dy):\n",
    "        model.train()\n",
    "        yhat = model(dx)\n",
    "        loss = loss_fn(dy, yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rewrite our `train_linear_model` function to take advantage of the `make_train_step_function` abstraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(model, dx, dy):\n",
    "    lr = 1e-1\n",
    "    EPOCHS = 1000\n",
    "    loss_fn = MSELoss(reduction='mean')\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    train_step = make_train_step_function(model, loss_fn, optimizer)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        loss = train_step(dx, dy)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Linear(1, 1)).to(device)\n",
    "train_linear_model(model, dx_train_tensor, dy_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing the data for our models\n",
    "\n",
    "Pytorch provides a number of helper classes and methods for dealing with data. Data isn't as simple as just a couple of arrays as we initially created at the start of this tutorial. Rather, there are operations like splitting data into training and validation datasets and splitting each one of those into a set of mini-batches that can be downloaded into your GPU's RAM (because the entire dataset won't fit at once for anything significant).\n",
    "\n",
    "A [Dataset](https://pytorch.org/docs/stable/data.html) is a base type that you can use for definining your own custom dataset. Below is a simple implementation of a `CustomDataset` that stores all data in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.dx = x_tensor\n",
    "        self.dy = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.dx[index], self.dy[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `CustomDataset` to load our training tensors and display some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7713],\n",
       "         [0.0636],\n",
       "         [0.8631],\n",
       "         [0.0254],\n",
       "         [0.7320]]), tensor([[2.4745],\n",
       "         [1.1928],\n",
       "         [2.9128],\n",
       "         [1.0785],\n",
       "         [2.4732]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = CustomDataset(dx_train_tensor, dy_train_tensor)\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the same functionality if you use the built-in `TensorDataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7713],\n",
       "         [0.0636],\n",
       "         [0.8631],\n",
       "         [0.0254],\n",
       "         [0.7320]]), tensor([[2.4745],\n",
       "         [1.1928],\n",
       "         [2.9128],\n",
       "         [1.0785],\n",
       "         [2.4732]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_data = TensorDataset(dx_train_tensor, dy_train_tensor)\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real value provided by representing your data as a `Dataset` comes when you combine it with a `DataLoader`. This type lets you easily implement __mini-batch gradient descent__ where only a subset of the dataset can fit in your GPU RAM at a time.\n",
    "\n",
    "The `train_loader` object created below behaves like a Python iterator, so we can use the `next` function to retrieve the next mini-batch containing 16 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2713],\n",
       "         [0.2809],\n",
       "         [0.3252],\n",
       "         [0.1834],\n",
       "         [0.7713],\n",
       "         [0.1705],\n",
       "         [0.3110],\n",
       "         [0.7722],\n",
       "         [0.7081],\n",
       "         [0.4938],\n",
       "         [0.8872],\n",
       "         [0.7320],\n",
       "         [0.2588],\n",
       "         [0.0344],\n",
       "         [0.5427],\n",
       "         [0.2921]]), tensor([[1.5105],\n",
       "         [1.5846],\n",
       "         [1.7291],\n",
       "         [1.4637],\n",
       "         [2.4745],\n",
       "         [1.3585],\n",
       "         [1.5245],\n",
       "         [2.4208],\n",
       "         [2.3660],\n",
       "         [1.9060],\n",
       "         [2.8708],\n",
       "         [2.4732],\n",
       "         [1.5967],\n",
       "         [1.1831],\n",
       "         [2.2161],\n",
       "         [1.5848]])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can implement mini-batch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_step = make_train_step_function(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for dx_batch, dy_batch in train_loader:\n",
    "        # In the case of GPU machines, we need to move the mini-batch to\n",
    "        # the device RAM first before we can train\n",
    "        dx_batch = dx_batch.to(device)\n",
    "        dy_batch = dy_batch.to(device)\n",
    "        \n",
    "        loss = train_step(dx_batch, dy_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our model\n",
    "\n",
    "Up until now, we've been looking at only training our model, not validating the results of the model against the validation dataset (perhaps I should go back and do this manually as well both from scratch and using numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "dx_tensor = torch.from_numpy(dx).float()\n",
    "dy_tensor = torch.from_numpy(dy).float()\n",
    "\n",
    "dataset = TensorDataset(dx_tensor, dy_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data prepared, next let's write both a training and a validation loop for our mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_step = make_train_step_function(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for dx_batch, dy_batch in train_loader:\n",
    "        dx_batch = dx_batch.to(device)\n",
    "        dy_batch = dy_batch.to(device)\n",
    "        loss = train_step(dx_batch, dy_batch)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for dx_val, dy_val in val_loader:\n",
    "            dx_val = dx_val.to(device)\n",
    "            dy_val = dy_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(dx_val)\n",
    "            val_loss = loss_fn(dy_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BUT__ I don't know how to visualize how well the validation losses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
